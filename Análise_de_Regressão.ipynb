{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Análise de Regressão.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMR0POMcjtbbi4jabV2ZpY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aboutdouglas/Aprendizado_Maquina/blob/main/An%C3%A1lise_de_Regress%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando LinearRegression()"
      ],
      "metadata": {
        "id": "SgxCXuJg9FxT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBCAmPj0T4VP",
        "outputId": "b72f3065-020e-43e9-b144-9d091f072e45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.00\n",
            "R2: 1.00\n"
          ]
        }
      ],
      "source": [
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# gerando os dados\n",
        "X = [[0,0], [1,1], [2,2]]\n",
        "Y = [0, 1, 2]\n",
        "\n",
        "# criação do modelo com o Método MMQ\n",
        "clf = linear_model.LinearRegression()\n",
        "\n",
        "# treinando o modelo\n",
        "clf.fit(X, Y)\n",
        "\n",
        "# exibe os coeficientes do modelo\n",
        "clf.coef_\n",
        "\n",
        "# gera previsões para outros valores\n",
        "Y_pred = clf.predict(X)\n",
        "\n",
        "# avalia o erro das previsões\n",
        "print(f'MSE: {mean_squared_error(Y, Y_pred):.2f}')\n",
        "print(f'R2: {r2_score(Y, Y_pred):.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizando SGDRegressor"
      ],
      "metadata": {
        "id": "MYKMPw9D9ZSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# gerando os dados\n",
        "X = [[0, 0], [1, 1], [2, 2]]\n",
        "Y = [0, 1, 2]\n",
        "\n",
        "# criação do modelo com o Método MMQ\n",
        "clf = linear_model.SGDRegressor(alpha=0.1, n_iter_no_change=20)\n",
        "\n",
        "# treina o modelo \n",
        "clf.fit (X, Y) \n",
        "\n",
        "# exibe os coeficientes do modelo\n",
        "clf.coef_\n",
        "\n",
        "# gera previsões para outros valores\n",
        "Y_pred = clf.predict(X)\n",
        "\n",
        "# avalia o erro das previsões\n",
        "print(f'MSE: {mean_squared_error(Y, Y_pred):.2f}')\n",
        "print(f'R2: {r2_score(Y, Y_pred):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7ZY6BNl83ns",
        "outputId": "164b7efa-78ee-4938-fb6e-ead2349704e2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.03\n",
            "R2: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No Scilearn-kit, esse processo é idêntico aos demais. O objeto é chamado SGDRegressor e é necessário ajustar o lambda (alpha) e o ElasticNet, o parâmetro de mistura l1_ratio. Para l1_ratio = 0, a penalidade é uma penalidade L2. Para l1_ratio = 1, é uma penalidade L1. Para 0 < l1_ratio < 1, a penalidade é uma combinação de L1 e L2."
      ],
      "metadata": {
        "id": "Q30ThhgwGehv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
        "\n",
        "# gerando os dados\n",
        "X = [[0, 0], [1, 1], [2, 2]]\n",
        "Y = [0, 1, 2]\n",
        "\n",
        "# Lasso - criação da Regressão Lasso com parâmetro lambda\n",
        "lasso = Lasso(alpha=0.1)\n",
        "lasso.fit(X, Y)\n",
        "y_pred_lasso = lasso.predict(X)\n",
        "r2_score_lasso = r2_score(Y, y_pred_lasso)\n",
        "\n",
        "# Ridge\n",
        "ridge = Ridge(alpha=0.1)\n",
        "y_pred_ridge = ridge.fit(X, Y)\n",
        "y_pred_ridge = ridge.predict(X)\n",
        "r2_score_ridge = r2_score(Y, y_pred_ridge)\n",
        "\n",
        "# ElasticNet\n",
        "enet = ElasticNet(alpha=0.1, l1_ratio=0.7)\n",
        "y_pred_enet = enet.fit(X, Y)\n",
        "y_pred_enet = enet.predict(X)\n",
        "r2_score_enet = r2_score(Y, y_pred_enet)\n",
        "\n",
        "print(f'r^2 enet: {r2_score_enet:.2f}')\n",
        "print(f'r^2 lasso: {r2_score_lasso:.2f}')\n",
        "print(f'r^2 ridge: {r2_score_ridge:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwaCuCihGlHw",
        "outputId": "16e05c1c-1d19-40d3-f6d2-9f70dbcfa1dd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r^2 enet: 0.98\n",
            "r^2 lasso: 0.98\n",
            "r^2 ridge: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Árvore de Regressão**"
      ],
      "metadata": {
        "id": "QYMTn_VOqeQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A principal diferença entre Árvore de Decisão e Árvore de Regressão é o critério de divisão: enquanto a AD busca escolher testes a partir de uma medida de \"pureza\" dos seus nós, a AR busca escolhe testes que minimizem o MSE da subárvore. Esse processo de divisão é realizado até que um critério de parada definido pelo usuário seja alcançado. Por exemplo: até que o número de observações por nó seja menor que 50.\n",
        "\n",
        "No Scilearn-kit, esse processo é idêntico à AD. A diferença está na necessidade de ajustar o critério de divisão (criterion, cujo default é o MSE) e, no caso das Árvores de Regressão, a quantidade de estimadores utilizados (n_estimators, cujo valor default é 100). O exemplo a seguir mostra esse processo na prática."
      ],
      "metadata": {
        "id": "mqPZmZAZqm12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# gerando os dados\n",
        "X = [[0, 0], [1, 1], [2, 2]]\n",
        "Y = [0, 1, 2]\n",
        "\n",
        "# criação do modelo com Árvore de Regressão\n",
        "clfT = DecisionTreeRegressor(criterion='squared_error')\n",
        "clfF = RandomForestRegressor(criterion='squared_error', n_estimators=10)\n",
        "\n",
        "# treina o modelo\n",
        "clfT.fit(X, Y)\n",
        "clfF.fit(X, Y)\n",
        "\n",
        "# gera previsões para outros valores\n",
        "Y_pred_T = clfT.predict(X)\n",
        "Y_pred_F = clfF.predict(X)\n",
        "\n",
        "# avalia o erro das previsões\n",
        "print(f'MSE Árvore: {mean_squared_error(Y, Y_pred_T):.2f}')\n",
        "print(f'MSE Floresta: {mean_squared_error(Y, Y_pred_F):.2f}')\n",
        "print(f'R2: {r2_score(Y, Y_pred_T):.2f}')\n",
        "print(f'R2: {r2_score(Y, Y_pred_F):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Nhdil1Aqu7U",
        "outputId": "98a3ab70-ac16-4d18-bdee-874184438fd6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE Árvore: 0.00\n",
            "MSE Floresta: 0.10\n",
            "R2: 1.00\n",
            "R2: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Suport Vector Regression**"
      ],
      "metadata": {
        "id": "j9JXDQXd4o_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As Support Vector Machines (SVMs) são bem conhecidas em problemas de classificação. O uso de SVMs na regressão não é tão bem documentado, no entanto. Esse tipo de modelo é conhecido como Support Vector Regression (SVR). A diferença entre a SVM e a SVR é que a SVR procura encontrar os vetores de suporte que descrevem os dados, e não que os separam em regiões"
      ],
      "metadata": {
        "id": "5G8uZ3Gy4xBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da mesma forma, como na classificação, há uma motivação para buscar e otimizar os limites de generalização dados para a regressão. Na regressão, além do termo de regularização, podemos operar com outros dois parâmetros: **gama** e **épsilon**.\n",
        "\n",
        "O parâmetro gama define o quão longe a influência de um único exemplo de treinamento alcança, como um raio que identifica quais amostras devem ser consideradas. Com um valor baixo para o parâmetro gama, pontos distantes das possíveis linhas de separação são considerados no cálculo da regressão. O parâmetro épsilon tem uma função para fornecer um erro tolerável do modelo de regressão. Depende dos problemas. Se precisarmos de um erro tolerável alto, podemos aumentar a banda intensiva épsilon e vice-versa.\n",
        "\n",
        "No Scilearn-kit, esse processo é identico à SVM. A diferença está na presença da variável épsilon. O exemplo a seguir mostra esse processo na prática."
      ],
      "metadata": {
        "id": "nxvOo_e05SZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# gerando os dados\n",
        "X = [[0, 0], [1, 1], [2, 2]]\n",
        "Y = [0, 1, 2]\n",
        "\n",
        "# criação do modelo com a SVR\n",
        "svr = SVR(C=1.0, epsilon=0.2)\n",
        "svr.fit(X, Y)\n",
        "\n",
        "# gera previsões para outros valores\n",
        "Y_pred = svr.predict(X)\n",
        "\n",
        "# avalia o erro das previsões\n",
        "print(f'MSE: {mean_squared_error(Y, Y_pred):.2f}')\n",
        "print(f'R2: {r2_score(Y, Y_pred):.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jV2YRdjw4oXn",
        "outputId": "344c9119-6161-4856-85f0-dced5b762b1d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.03\n",
            "R2: 0.96\n"
          ]
        }
      ]
    }
  ]
}